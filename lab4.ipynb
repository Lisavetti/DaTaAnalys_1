{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeaaVDMzQAMfuj7PNNQgjU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lisavetti/DaTaAnalys_1/blob/main/lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vw2-qybKWDE7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization, MultiHeadAttention, LayerNormalization, Dense, Embedding, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "import re\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = 'ukr.txt'"
      ],
      "metadata": {
        "id": "fpY1aDrfW_Dj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare the data\n",
        "with open(\"ukr.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, ukr, _ = line.strip().split(\"\\t\")\n",
        "    ukr = \"[start] \" + ukr + \" [end]\"\n",
        "    text_pairs.append((eng, ukr))"
      ],
      "metadata": {
        "id": "L8uG2XObXE4O"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOJjAAqlXxP1",
        "outputId": "579af0f5-35d0-4075-b186-7c9dc68d452a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Who was Tom afraid of?', '[start] Кого боявся Том? [end]')\n",
            "('What a hot day!', '[start] Який спекотний день! [end]')\n",
            "('Why are you so good at cooking?', '[start] Чому ти так добре готуєш? [end]')\n",
            "(\"I'm still not sure I can help you.\", '[start] Я й досі не певен, що можу вам допомогти. [end]')\n",
            "(\"That's Tom's watch.\", '[start] Це годинник Тома. [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
        "\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edCg3tFEX5rM",
        "outputId": "2bdd5f34-90ab-43c4-8eb5-a0e82a7c18cc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "158705 total pairs\n",
            "111095 training pairs\n",
            "23805 validation pairs\n",
            "23805 test pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Vectorization\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "eng_vectorization = TextVectorization(max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length)\n",
        "ukr_vectorization = TextVectorization(max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length + 1, standardize=custom_standardization)\n",
        "\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_ukr_texts = [pair[1] for pair in train_pairs]\n",
        "eng_vectorization.adapt(train_eng_texts)\n",
        "ukr_vectorization.adapt(train_ukr_texts)"
      ],
      "metadata": {
        "id": "gZsTv6ExYCNx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Encoder Layer\n",
        "class TransformerEncoderLayer(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = tf.keras.Sequential([Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.attention(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.dense_proj(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "JTfQHt0iYKf_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Decoder Layer\n",
        "class TransformerDecoderLayer(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        self.attention1 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention2 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = tf.keras.Sequential([Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "        self.dropout3 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, enc_output, training):\n",
        "        attn_output1 = self.attention1(inputs, inputs)\n",
        "        attn_output1 = self.dropout1(attn_output1, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output1)\n",
        "        attn_output2 = self.attention2(out1, enc_output)\n",
        "        attn_output2 = self.dropout2(attn_output2, training=training)\n",
        "        out2 = self.layernorm2(out1 + attn_output2)\n",
        "        ffn_output = self.dense_proj(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        return self.layernorm3(out2 + ffn_output)\n"
      ],
      "metadata": {
        "id": "I0xtIhi3YOV2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Transformer model\n",
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = layers.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = Embedding(input_dim=vocab_size, output_dim=embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoderLayer(embed_dim, num_heads, latent_dim)(x)\n",
        "\n",
        "decoder_inputs = layers.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = layers.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = Embedding(input_dim=vocab_size, output_dim=embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoderLayer(embed_dim, num_heads, latent_dim)(x, encoder_outputs)\n",
        "decoder_outputs = Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "transformer = Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\")\n",
        "\n",
        "# Compile the model\n",
        "transformer.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Prepare datasets\n",
        "def format_dataset(eng, ukr):\n",
        "    eng = eng_vectorization(eng)\n",
        "    ukr = ukr_vectorization(ukr)\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": ukr[:, :-1]}, ukr[:, 1:])\n",
        "\n",
        "# Reduce the number of epochs\n",
        "epochs = 3\n",
        "\n",
        "# Increase batch size if your hardware can handle it\n",
        "batch_size = 128\n",
        "\n",
        "def make_dataset(pairs, batch_size):\n",
        "    eng_texts, ukr_texts = zip(*pairs)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(eng_texts), list(ukr_texts)))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "train_ds = make_dataset(train_pairs, batch_size)\n",
        "val_ds = make_dataset(val_pairs, batch_size)\n",
        "\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haJ20sdEYZvw",
        "outputId": "e4248a46-3634-4a54-fad6-a3af7503d8bf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "868/868 [==============================] - 4870s 6s/step - loss: 1.4914 - accuracy: 0.8074 - val_loss: 3.1767 - val_accuracy: 0.7121\n",
            "Epoch 2/3\n",
            "868/868 [==============================] - 4865s 6s/step - loss: 1.1184 - accuracy: 0.8466 - val_loss: 0.6312 - val_accuracy: 0.8908\n",
            "Epoch 3/3\n",
            "868/868 [==============================] - 4868s 6s/step - loss: 0.6907 - accuracy: 0.8894 - val_loss: 0.4837 - val_accuracy: 0.9088\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ef163d9cfa0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Assuming ukr_vectorization is for Ukrainian and eng_vectorization is for English\n",
        "ukr_vocab = ukr_vectorization.get_vocabulary()\n",
        "ukr_index_lookup = dict(zip(range(len(ukr_vocab)), ukr_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = ukr_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = ukr_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence.replace(\"[start]\", \"\").strip()\n",
        "\n",
        "# Test the model with a few examples\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]  # Assuming test_pairs is a list of English-Ukrainian pairs\n",
        "for _ in range(5):  # Testing with 5 random sentences\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequence(input_sentence)\n",
        "    print(f\"EN: {input_sentence}\\nUKR: {translated}\\n\")\n",
        "\n",
        "# Example translation\n",
        "print(\"Example translation of 'hi':\")\n",
        "print(decode_sequence('hi'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eULDEPwgpoBw",
        "outputId": "c7a0b4d5-a821-413d-a60f-895aadcd3cac"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EN: Is this book yours?\n",
            "UKR: Так твоя книжка [end]\n",
            "\n",
            "EN: I'm surprised at your behavior.\n",
            "UKR: Боюся ваш улюблений напій [end]\n",
            "\n",
            "EN: Let him do it.\n",
            "UKR: Дайно ним [end]\n",
            "\n",
            "EN: Learning a foreign language is a waste of time.\n",
            "UKR: Час вивчати мови [end]\n",
            "\n",
            "EN: Nobody saw anything.\n",
            "UKR: Ніхто нічого бачив [end]\n",
            "\n",
            "Example translation of 'hi':\n",
            "Привіт Томе [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbHI833sdhws",
        "outputId": "f2222963-8772-4e07-f9d7-e42578941da7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentencepiece\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcLyS9c-fyGQ",
        "outputId": "e28635e2-d965-4712-8b6d-76df7426789a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Список моделей\n",
        "uk_models = [\n",
        "    \"csebuetnlp/mT5_multilingual_XLSum\", # mT5 для багатомовної сумаризації, включаючи українську\n",
        "    \"Geotrend/bert-base-uk-cased\",        # BERT специфічно для української мови\n",
        "    \"IlyaGusev/rut5_base_sum_gazeta_uk\",  # T5 для сумаризації українських новин\n",
        "    \"sberbank-ai/ruRoberta-large\"         # RoBERTa для російської, але може бути застосована до української\n",
        "]\n",
        "\n",
        "# Створення пайплайнів\n",
        "pipelines = {}\n",
        "for model in uk_models:\n",
        "    try:\n",
        "        if \"sum\" in model:\n",
        "            pipelines[model] = pipeline(\"summarization\", model=model)\n",
        "        else:\n",
        "            pipelines[model] = pipeline(\"fill-mask\", model=model)\n",
        "    except Exception as e:\n",
        "        print(f\"Не вдалося створити пайплайн для моделі {model}: {e}\")\n",
        "\n",
        "# Приклад використання пайплайну для сумаризації\n",
        "text_for_summarization = \"Новий рік принесе щастя і мир\"\n",
        "try:\n",
        "    summarized_text = pipelines[\"csebuetnlp/mT5_multilingual_XLSum\"](text_for_summarization)\n",
        "    print(\"Сумаризований текст:\", summarized_text)\n",
        "except Exception as e:\n",
        "    print(f\"Помилка при сумаризації тексту: {e}\")\n",
        "\n",
        "# Приклад використання пайплайну для заповнення пропущених слів\n",
        "text_for_fill_mask = \"Вітаю з [MASK] роком\"\n",
        "try:\n",
        "    filled_text = pipelines[\"Geotrend/bert-base-uk-cased\"](text_for_fill_mask)\n",
        "    print(\"Текст із заповненими пропусками:\", filled_text)\n",
        "except Exception as e:\n",
        "    print(f\"Помилка при заповненні пропущених слів: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRYosZThdhrB",
        "outputId": "fee08228-76d1-45bc-d240-cb66dbb410cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Не вдалося створити пайплайн для моделі csebuetnlp/mT5_multilingual_XLSum: Couldn't instantiate the backend tokenizer from one of: \n",
            "(1) a `tokenizers` library serialization file, \n",
            "(2) a slow tokenizer instance to convert or \n",
            "(3) an equivalent slow tokenizer class to instantiate and convert. \n",
            "You need to have sentencepiece installed to convert a slow tokenizer to a fast one.\n",
            "Не вдалося створити пайплайн для моделі IlyaGusev/rut5_base_sum_gazeta_uk: IlyaGusev/rut5_base_sum_gazeta_uk is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
            "Помилка при сумаризації тексту: 'csebuetnlp/mT5_multilingual_XLSum'\n",
            "Текст із заповненими пропусками: [{'score': 0.26641497015953064, 'token': 10709, 'token_str': 'новим', 'sequence': 'Вітаю з новим роком'}, {'score': 0.2534734308719635, 'token': 8223, 'token_str': 'цим', 'sequence': 'Вітаю з цим роком'}, {'score': 0.09071118384599686, 'token': 6850, 'token_str': 'великим', 'sequence': 'Вітаю з великим роком'}, {'score': 0.053628019988536835, 'token': 6007, 'token_str': 'іншими', 'sequence': 'Вітаю з іншими роком'}, {'score': 0.048481278121471405, 'token': 3783, 'token_str': 'таким', 'sequence': 'Вітаю з таким роком'}]\n"
          ]
        }
      ]
    }
  ]
}