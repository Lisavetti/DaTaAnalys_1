{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGWUmP7YMI6TJDGWMbo0/+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lisavetti/DaTaAnalys_1/blob/main/lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vw2-qybKWDE7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization, MultiHeadAttention, LayerNormalization, Dense, Embedding, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "import re\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_file = 'ukr.txt'"
      ],
      "metadata": {
        "id": "fpY1aDrfW_Dj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and prepare the data\n",
        "with open(\"ukr.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, ukr, _ = line.strip().split(\"\\t\")\n",
        "    ukr = \"[start] \" + ukr + \" [end]\"\n",
        "    text_pairs.append((eng, ukr))"
      ],
      "metadata": {
        "id": "L8uG2XObXE4O"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOJjAAqlXxP1",
        "outputId": "579af0f5-35d0-4075-b186-7c9dc68d452a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Who was Tom afraid of?', '[start] Кого боявся Том? [end]')\n",
            "('What a hot day!', '[start] Який спекотний день! [end]')\n",
            "('Why are you so good at cooking?', '[start] Чому ти так добре готуєш? [end]')\n",
            "(\"I'm still not sure I can help you.\", '[start] Я й досі не певен, що можу вам допомогти. [end]')\n",
            "(\"That's Tom's watch.\", '[start] Це годинник Тома. [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]\n",
        "\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edCg3tFEX5rM",
        "outputId": "2bdd5f34-90ab-43c4-8eb5-a0e82a7c18cc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "158705 total pairs\n",
            "111095 training pairs\n",
            "23805 validation pairs\n",
            "23805 test pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Vectorization\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
        "\n",
        "eng_vectorization = TextVectorization(max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length)\n",
        "ukr_vectorization = TextVectorization(max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length + 1, standardize=custom_standardization)\n",
        "\n",
        "train_eng_texts = [pair[0] for pair in train_pairs]\n",
        "train_ukr_texts = [pair[1] for pair in train_pairs]\n",
        "eng_vectorization.adapt(train_eng_texts)\n",
        "ukr_vectorization.adapt(train_ukr_texts)"
      ],
      "metadata": {
        "id": "gZsTv6ExYCNx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Encoder Layer\n",
        "class TransformerEncoderLayer(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerEncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = tf.keras.Sequential([Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.attention(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.dense_proj(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "JTfQHt0iYKf_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformer Decoder Layer\n",
        "class TransformerDecoderLayer(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerDecoderLayer, self).__init__()\n",
        "        self.attention1 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention2 = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = tf.keras.Sequential([Dense(ff_dim, activation=\"relu\"), Dense(embed_dim)])\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "        self.dropout3 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, enc_output, training):\n",
        "        attn_output1 = self.attention1(inputs, inputs)\n",
        "        attn_output1 = self.dropout1(attn_output1, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output1)\n",
        "        attn_output2 = self.attention2(out1, enc_output)\n",
        "        attn_output2 = self.dropout2(attn_output2, training=training)\n",
        "        out2 = self.layernorm2(out1 + attn_output2)\n",
        "        ffn_output = self.dense_proj(out2)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        return self.layernorm3(out2 + ffn_output)\n"
      ],
      "metadata": {
        "id": "I0xtIhi3YOV2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Transformer model\n",
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = layers.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = Embedding(input_dim=vocab_size, output_dim=embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoderLayer(embed_dim, num_heads, latent_dim)(x)\n",
        "\n",
        "decoder_inputs = layers.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = layers.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = Embedding(input_dim=vocab_size, output_dim=embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoderLayer(embed_dim, num_heads, latent_dim)(x, encoder_outputs)\n",
        "decoder_outputs = Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "transformer = Model([encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\")\n",
        "\n",
        "# Compile the model\n",
        "transformer.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Prepare datasets\n",
        "def format_dataset(eng, ukr):\n",
        "    eng = eng_vectorization(eng)\n",
        "    ukr = ukr_vectorization(ukr)\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": ukr[:, :-1]}, ukr[:, 1:])\n",
        "\n",
        "# Reduce the number of epochs\n",
        "epochs = 3\n",
        "\n",
        "# Increase batch size if your hardware can handle it\n",
        "batch_size = 128\n",
        "\n",
        "def make_dataset(pairs, batch_size):\n",
        "    eng_texts, ukr_texts = zip(*pairs)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((list(eng_texts), list(ukr_texts)))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "train_ds = make_dataset(train_pairs, batch_size)\n",
        "val_ds = make_dataset(val_pairs, batch_size)\n",
        "\n",
        "transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haJ20sdEYZvw",
        "outputId": "e4248a46-3634-4a54-fad6-a3af7503d8bf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "868/868 [==============================] - 4870s 6s/step - loss: 1.4914 - accuracy: 0.8074 - val_loss: 3.1767 - val_accuracy: 0.7121\n",
            "Epoch 2/3\n",
            "868/868 [==============================] - 4865s 6s/step - loss: 1.1184 - accuracy: 0.8466 - val_loss: 0.6312 - val_accuracy: 0.8908\n",
            "Epoch 3/3\n",
            "868/868 [==============================] - 4868s 6s/step - loss: 0.6907 - accuracy: 0.8894 - val_loss: 0.4837 - val_accuracy: 0.9088\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ef163d9cfa0>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Assuming ukr_vectorization is for Ukrainian and eng_vectorization is for English\n",
        "ukr_vocab = ukr_vectorization.get_vocabulary()\n",
        "ukr_index_lookup = dict(zip(range(len(ukr_vocab)), ukr_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = eng_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = ukr_vectorization([decoded_sentence])[:, :-1]\n",
        "        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n",
        "\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = ukr_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence.replace(\"[start]\", \"\").strip()\n",
        "\n",
        "# Test the model with a few examples\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]  # Assuming test_pairs is a list of English-Ukrainian pairs\n",
        "for _ in range(5):  # Testing with 5 random sentences\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    translated = decode_sequence(input_sentence)\n",
        "    print(f\"EN: {input_sentence}\\nUKR: {translated}\\n\")\n",
        "\n",
        "# Example translation\n",
        "print(\"Example translation of 'hi':\")\n",
        "print(decode_sequence('hi'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eULDEPwgpoBw",
        "outputId": "c7a0b4d5-a821-413d-a60f-895aadcd3cac"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EN: Is this book yours?\n",
            "UKR: Так твоя книжка [end]\n",
            "\n",
            "EN: I'm surprised at your behavior.\n",
            "UKR: Боюся ваш улюблений напій [end]\n",
            "\n",
            "EN: Let him do it.\n",
            "UKR: Дайно ним [end]\n",
            "\n",
            "EN: Learning a foreign language is a waste of time.\n",
            "UKR: Час вивчати мови [end]\n",
            "\n",
            "EN: Nobody saw anything.\n",
            "UKR: Ніхто нічого бачив [end]\n",
            "\n",
            "Example translation of 'hi':\n",
            "Привіт Томе [end]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbHI833sdhws",
        "outputId": "f2222963-8772-4e07-f9d7-e42578941da7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentencepiece\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcLyS9c-fyGQ",
        "outputId": "e28635e2-d965-4712-8b6d-76df7426789a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Список моделей\n",
        "uk_models = [\n",
        "    \"csebuetnlp/mT5_multilingual_XLSum\", # mT5 для багатомовної сумаризації, включаючи українську\n",
        "    \"Geotrend/bert-base-uk-cased\",        # BERT специфічно для української мови\n",
        "    \"IlyaGusev/rut5_base_sum_gazeta_uk\",  # T5 для сумаризації українських новин\n",
        "    \"sberbank-ai/ruRoberta-large\"         # RoBERTa для російської, але може бути застосована до української\n",
        "]\n",
        "\n",
        "# Створення пайплайнів\n",
        "pipelines = {}\n",
        "for model in uk_models:\n",
        "    try:\n",
        "        if \"sum\" in model:\n",
        "            pipelines[model] = pipeline(\"summarization\", model=model)\n",
        "        else:\n",
        "            pipelines[model] = pipeline(\"fill-mask\", model=model)\n",
        "    except Exception as e:\n",
        "        print(f\"Не вдалося створити пайплайн для моделі {model}: {e}\")\n",
        "\n",
        "# Приклад використання пайплайну для сумаризації\n",
        "text_for_summarization = \"Новий рік принесе щастя і мир\"\n",
        "try:\n",
        "    summarized_text = pipelines[\"csebuetnlp/mT5_multilingual_XLSum\"](text_for_summarization)\n",
        "    print(\"Сумаризований текст:\", summarized_text)\n",
        "except Exception as e:\n",
        "    print(f\"Помилка при сумаризації тексту: {e}\")\n",
        "\n",
        "# Приклад використання пайплайну для заповнення пропущених слів\n",
        "text_for_fill_mask = \"Вітаю з [MASK] роком\"\n",
        "try:\n",
        "    filled_text = pipelines[\"Geotrend/bert-base-uk-cased\"](text_for_fill_mask)\n",
        "    print(\"Текст із заповненими пропусками:\", filled_text)\n",
        "except Exception as e:\n",
        "    print(f\"Помилка при заповненні пропущених слів: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRYosZThdhrB",
        "outputId": "fee08228-76d1-45bc-d240-cb66dbb410cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Не вдалося створити пайплайн для моделі csebuetnlp/mT5_multilingual_XLSum: Couldn't instantiate the backend tokenizer from one of: \n",
            "(1) a `tokenizers` library serialization file, \n",
            "(2) a slow tokenizer instance to convert or \n",
            "(3) an equivalent slow tokenizer class to instantiate and convert. \n",
            "You need to have sentencepiece installed to convert a slow tokenizer to a fast one.\n",
            "Не вдалося створити пайплайн для моделі IlyaGusev/rut5_base_sum_gazeta_uk: IlyaGusev/rut5_base_sum_gazeta_uk is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
            "Помилка при сумаризації тексту: 'csebuetnlp/mT5_multilingual_XLSum'\n",
            "Текст із заповненими пропусками: [{'score': 0.26641497015953064, 'token': 10709, 'token_str': 'новим', 'sequence': 'Вітаю з новим роком'}, {'score': 0.2534734308719635, 'token': 8223, 'token_str': 'цим', 'sequence': 'Вітаю з цим роком'}, {'score': 0.09071118384599686, 'token': 6850, 'token_str': 'великим', 'sequence': 'Вітаю з великим роком'}, {'score': 0.053628019988536835, 'token': 6007, 'token_str': 'іншими', 'sequence': 'Вітаю з іншими роком'}, {'score': 0.048481278121471405, 'token': 3783, 'token_str': 'таким', 'sequence': 'Вітаю з таким роком'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9_OqEYpHuHD",
        "outputId": "90ddfe32-0263-4685-f09e-f31e75215879"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "image_size = 28\n",
        "nz = 100  # Size of z latent vector (i.e., size of generator input)\n",
        "ngf = 64  # Size of feature maps in generator\n",
        "ndf = 64  # Size of feature maps in discriminator\n",
        "num_epochs = 5\n",
        "lr = 0.0002\n",
        "beta1 = 0.5\n",
        "\n",
        "# Check for device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Image preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "dataset = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Define the Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # Input is Z, going into a convolution\n",
        "            nn.ConvTranspose2d(nz, ngf * 4, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            # State size. (ngf*4) x 4 x 4\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            # State size. (ngf*2) x 7 x 7\n",
        "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            # State size. (ngf) x 14 x 14\n",
        "            nn.ConvTranspose2d(ngf, 1, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            # State size. (1) x 28 x 28\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n",
        "\n",
        "# Define the Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # Input size. (1) x 28 x 28\n",
        "            nn.Conv2d(1, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # State size. (ndf) x 14 x 14\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # State size. (ndf*2) x 7 x 7\n",
        "            nn.Conv2d(ndf * 2, 1, 7, 1, 0, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input).view(-1, 1).squeeze(1)\n",
        "\n",
        "# Create the generator and discriminator\n",
        "netG = Generator().to(device)\n",
        "netD = Discriminator().to(device)\n",
        "\n",
        "# Initialize BCELoss function\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# Create batch of latent vectors\n",
        "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
        "\n",
        "# Establish convention for real and fake labels\n",
        "real_label = 1\n",
        "fake_label = 0\n",
        "\n",
        "# Setup Adam optimizers\n",
        "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    for i, data in enumerate(dataloader, 0):\n",
        "        # Train with all-real batch\n",
        "        netD.zero_grad()\n",
        "        real_cpu = data[0].to(device)\n",
        "        b_size = real_cpu.size(0)\n",
        "        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "        output = netD(real_cpu).view(-1)\n",
        "        errD_real = criterion(output, label)\n",
        "        errD_real.backward()\n",
        "        D_x = output.mean().item()\n",
        "\n",
        "        # Train with all-fake batch\n",
        "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "        fake = netG(noise)\n",
        "        label.fill_(fake_label).type(torch.float)\n",
        "        output = netD(fake.detach()).view(-1)\n",
        "        errD_fake = criterion(output, label)\n",
        "        errD_fake.backward()\n",
        "        D_G_z1 = output.mean().item()\n",
        "        errD = errD_real + errD_fake\n",
        "        optimizerD.step()\n",
        "\n",
        "        # Update G network\n",
        "        netG.zero_grad()\n",
        "        label.fill_(real_label).type(torch.float)\n",
        "        output = netD(fake).view(-1)\n",
        "        errG = criterion(output, label)\n",
        "        errG.backward()\n",
        "        D_G_z2 = output.mean().item()\n",
        "        optimizerG.step()\n",
        "\n",
        "        # Output training stats\n",
        "        if i % 50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                  % (epoch, num_epochs, i, len(dataloader),\n",
        "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
        "\n",
        "        # Check how the generator is doing by saving G's output on fixed_noise\n",
        "        if (epoch % 1 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
        "            with torch.no_grad():\n",
        "                fake = netG(fixed_noise).detach().cpu()\n",
        "            save_image(fake, f'output_epoch_{epoch}.png', nrow=8, normalize=True)\n",
        "\n",
        "print(\"Training Finished.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Neiq2QTsIS5j",
        "outputId": "32b12126-a50b-4dc8-ee14-aa13b5fdfc0c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0/5][0/469]\tLoss_D: 1.3723\tLoss_G: 1.1063\tD(x): 0.4579\tD(G(z)): 0.4342 / 0.3396\n",
            "[0/5][50/469]\tLoss_D: 0.1411\tLoss_G: 3.4175\tD(x): 0.9439\tD(G(z)): 0.0759 / 0.0343\n",
            "[0/5][100/469]\tLoss_D: 0.7024\tLoss_G: 2.1098\tD(x): 0.7765\tD(G(z)): 0.3369 / 0.1382\n",
            "[0/5][150/469]\tLoss_D: 0.9629\tLoss_G: 1.3009\tD(x): 0.6121\tD(G(z)): 0.3478 / 0.2830\n",
            "[0/5][200/469]\tLoss_D: 0.8009\tLoss_G: 1.3481\tD(x): 0.6668\tD(G(z)): 0.3006 / 0.2751\n",
            "[0/5][250/469]\tLoss_D: 0.6211\tLoss_G: 2.3467\tD(x): 0.8150\tD(G(z)): 0.3283 / 0.1081\n",
            "[0/5][300/469]\tLoss_D: 0.6651\tLoss_G: 1.8665\tD(x): 0.7698\tD(G(z)): 0.3114 / 0.1701\n",
            "[0/5][350/469]\tLoss_D: 0.5416\tLoss_G: 1.8864\tD(x): 0.7617\tD(G(z)): 0.2232 / 0.1634\n",
            "[0/5][400/469]\tLoss_D: 0.7310\tLoss_G: 2.0892\tD(x): 0.8399\tD(G(z)): 0.4160 / 0.1352\n",
            "[0/5][450/469]\tLoss_D: 0.6749\tLoss_G: 1.5568\tD(x): 0.6722\tD(G(z)): 0.2224 / 0.2258\n",
            "[1/5][0/469]\tLoss_D: 0.8306\tLoss_G: 0.9983\tD(x): 0.5840\tD(G(z)): 0.2170 / 0.3858\n",
            "[1/5][50/469]\tLoss_D: 0.7757\tLoss_G: 1.1618\tD(x): 0.6755\tD(G(z)): 0.2942 / 0.3344\n",
            "[1/5][100/469]\tLoss_D: 0.7835\tLoss_G: 1.3609\tD(x): 0.6786\tD(G(z)): 0.2993 / 0.2726\n",
            "[1/5][150/469]\tLoss_D: 0.9791\tLoss_G: 0.7258\tD(x): 0.4867\tD(G(z)): 0.1947 / 0.4967\n",
            "[1/5][200/469]\tLoss_D: 0.9725\tLoss_G: 0.7294\tD(x): 0.5297\tD(G(z)): 0.2416 / 0.4979\n",
            "[1/5][250/469]\tLoss_D: 0.8469\tLoss_G: 1.2800\tD(x): 0.6608\tD(G(z)): 0.3243 / 0.2985\n",
            "[1/5][300/469]\tLoss_D: 1.1654\tLoss_G: 0.7320\tD(x): 0.3967\tD(G(z)): 0.1555 / 0.4957\n",
            "[1/5][350/469]\tLoss_D: 0.9556\tLoss_G: 1.6543\tD(x): 0.7452\tD(G(z)): 0.4609 / 0.2090\n",
            "[1/5][400/469]\tLoss_D: 1.0472\tLoss_G: 1.5941\tD(x): 0.7547\tD(G(z)): 0.5154 / 0.2193\n",
            "[1/5][450/469]\tLoss_D: 1.0382\tLoss_G: 1.4674\tD(x): 0.6997\tD(G(z)): 0.4757 / 0.2448\n",
            "[2/5][0/469]\tLoss_D: 1.1432\tLoss_G: 1.5780\tD(x): 0.7807\tD(G(z)): 0.5705 / 0.2242\n",
            "[2/5][50/469]\tLoss_D: 1.0370\tLoss_G: 1.2650\tD(x): 0.6494\tD(G(z)): 0.4304 / 0.3008\n",
            "[2/5][100/469]\tLoss_D: 1.1462\tLoss_G: 1.3439\tD(x): 0.6434\tD(G(z)): 0.4804 / 0.2789\n",
            "[2/5][150/469]\tLoss_D: 1.0231\tLoss_G: 1.0631\tD(x): 0.5674\tD(G(z)): 0.3395 / 0.3590\n",
            "[2/5][200/469]\tLoss_D: 1.0205\tLoss_G: 1.0766\tD(x): 0.5597\tD(G(z)): 0.3261 / 0.3567\n",
            "[2/5][250/469]\tLoss_D: 1.1001\tLoss_G: 0.8261\tD(x): 0.5354\tD(G(z)): 0.3472 / 0.4489\n",
            "[2/5][300/469]\tLoss_D: 1.1938\tLoss_G: 1.7840\tD(x): 0.7180\tD(G(z)): 0.5602 / 0.1838\n",
            "[2/5][350/469]\tLoss_D: 1.0462\tLoss_G: 1.2468\tD(x): 0.6695\tD(G(z)): 0.4537 / 0.3034\n",
            "[2/5][400/469]\tLoss_D: 1.0256\tLoss_G: 1.0011\tD(x): 0.5669\tD(G(z)): 0.3409 / 0.3827\n",
            "[2/5][450/469]\tLoss_D: 1.0593\tLoss_G: 1.2381\tD(x): 0.6127\tD(G(z)): 0.4123 / 0.3040\n",
            "[3/5][0/469]\tLoss_D: 1.3095\tLoss_G: 0.5111\tD(x): 0.3637\tD(G(z)): 0.1990 / 0.6124\n",
            "[3/5][50/469]\tLoss_D: 1.1503\tLoss_G: 0.7479\tD(x): 0.4956\tD(G(z)): 0.3275 / 0.4848\n",
            "[3/5][100/469]\tLoss_D: 1.0254\tLoss_G: 1.0776\tD(x): 0.5572\tD(G(z)): 0.3276 / 0.3567\n",
            "[3/5][150/469]\tLoss_D: 1.1476\tLoss_G: 0.8375\tD(x): 0.4867\tD(G(z)): 0.3063 / 0.4498\n",
            "[3/5][200/469]\tLoss_D: 1.2001\tLoss_G: 1.1700\tD(x): 0.6434\tD(G(z)): 0.5128 / 0.3269\n",
            "[3/5][250/469]\tLoss_D: 1.0887\tLoss_G: 1.1606\tD(x): 0.5790\tD(G(z)): 0.3801 / 0.3308\n",
            "[3/5][300/469]\tLoss_D: 1.0927\tLoss_G: 0.8702\tD(x): 0.5008\tD(G(z)): 0.3039 / 0.4307\n",
            "[3/5][350/469]\tLoss_D: 1.0750\tLoss_G: 1.2831\tD(x): 0.6405\tD(G(z)): 0.4471 / 0.2956\n",
            "[3/5][400/469]\tLoss_D: 1.1948\tLoss_G: 1.4944\tD(x): 0.7182\tD(G(z)): 0.5635 / 0.2350\n",
            "[3/5][450/469]\tLoss_D: 1.0786\tLoss_G: 0.9218\tD(x): 0.5309\tD(G(z)): 0.3300 / 0.4137\n",
            "[4/5][0/469]\tLoss_D: 1.1732\tLoss_G: 0.8784\tD(x): 0.5281\tD(G(z)): 0.3876 / 0.4332\n",
            "[4/5][50/469]\tLoss_D: 1.1820\tLoss_G: 0.8091\tD(x): 0.5052\tD(G(z)): 0.3565 / 0.4613\n",
            "[4/5][100/469]\tLoss_D: 1.2888\tLoss_G: 0.6631\tD(x): 0.4087\tD(G(z)): 0.2733 / 0.5291\n",
            "[4/5][150/469]\tLoss_D: 1.3604\tLoss_G: 0.6727\tD(x): 0.3457\tD(G(z)): 0.1935 / 0.5282\n",
            "[4/5][200/469]\tLoss_D: 1.1733\tLoss_G: 0.8958\tD(x): 0.5120\tD(G(z)): 0.3605 / 0.4300\n",
            "[4/5][250/469]\tLoss_D: 1.2297\tLoss_G: 1.4563\tD(x): 0.7481\tD(G(z)): 0.5742 / 0.2539\n",
            "[4/5][300/469]\tLoss_D: 1.1458\tLoss_G: 1.3067\tD(x): 0.6510\tD(G(z)): 0.4846 / 0.2872\n",
            "[4/5][350/469]\tLoss_D: 1.3678\tLoss_G: 0.5275\tD(x): 0.3625\tD(G(z)): 0.2287 / 0.6011\n",
            "[4/5][400/469]\tLoss_D: 1.1699\tLoss_G: 1.1506\tD(x): 0.6247\tD(G(z)): 0.4714 / 0.3363\n",
            "[4/5][450/469]\tLoss_D: 1.3162\tLoss_G: 1.7722\tD(x): 0.7096\tD(G(z)): 0.5863 / 0.1938\n",
            "Training Finished.\n"
          ]
        }
      ]
    }
  ]
}